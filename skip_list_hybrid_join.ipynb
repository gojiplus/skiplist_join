{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3158d5b9-409d-41ba-942c-dedbc3be871f",
   "metadata": {},
   "source": [
    "### Hybrid Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7540f63b-5b92-48e8-87c4-36fde60dcf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Medium Dataset Test ===\n",
      "Hybrid index build time: 0.051599 seconds\n",
      "Hybrid join time: 0.016645 seconds\n",
      "Total hybrid join time: 0.068244 seconds\n",
      "MT Hash table build time: 0.001468 seconds\n",
      "MT Hash join time: 0.006358 seconds\n",
      "Total MT hash join time: 0.007826 seconds\n",
      "Bloom filter + hash table build time: 0.185776 seconds\n",
      "Bloom filter join time: 0.082973 seconds\n",
      "Total Bloom filter join time: 0.268749 seconds\n",
      "Bloom filter hits: 8712, Actual matches: 8712, False positives: 0\n",
      "Bloom filter join result size: 20346\n",
      "Hybrid join result size: 20346\n",
      "Multi-threaded hash join result size: 20346\n",
      "\n",
      "=== Large Dataset Test ===\n",
      "Hybrid index build time: 0.307271 seconds\n",
      "Hybrid join time: 0.332649 seconds\n",
      "Total hybrid join time: 0.639920 seconds\n",
      "MT Hash table build time: 0.017525 seconds\n",
      "MT Hash join time: 0.118315 seconds\n",
      "Total MT hash join time: 0.135840 seconds\n",
      "Bloom filter + hash table build time: 0.852079 seconds\n",
      "Bloom filter join time: 1.007549 seconds\n",
      "Total Bloom filter join time: 1.859628 seconds\n",
      "Bloom filter hits: 86100, Actual matches: 86100, False positives: 0\n",
      "Bloom filter join result size: 199097\n",
      "Hybrid join result size: 199097\n",
      "Multi-threaded hash join result size: 199097\n",
      "\n",
      "=== Highly Skewed Data Distribution Test ===\n",
      "Hybrid index build time: 0.009282 seconds\n",
      "Hybrid join time: 1.601564 seconds\n",
      "Total hybrid join time: 1.610846 seconds\n",
      "MT Hash table build time: 0.001136 seconds\n",
      "MT Hash join time: 2.447479 seconds\n",
      "Total MT hash join time: 2.448615 seconds\n",
      "Bloom filter + hash table build time: 0.082512 seconds\n",
      "Bloom filter join time: 1.931642 seconds\n",
      "Total Bloom filter join time: 2.014154 seconds\n",
      "Bloom filter hits: 9152, Actual matches: 9152, False positives: 0\n",
      "Bloom filter join result size: 8091750\n",
      "Hybrid join result size: 8091750\n",
      "Multi-threaded hash join result size: 8091750\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class SkipNode:\n",
    "    \"\"\"A node in the Skip List\"\"\"\n",
    "    \n",
    "    def __init__(self, key: Any, value: Any, level: int):\n",
    "        self.key = key\n",
    "        self.value = value  # Could be a list of values if there are duplicates\n",
    "        self.forward = [None] * (level + 1)  # Array of pointers for each level\n",
    "        \n",
    "\n",
    "class SkipList:\n",
    "    \"\"\"Skip list implementation with search, insert, and delete operations\"\"\"\n",
    "    \n",
    "    def __init__(self, max_level: int = 16, p: float = 0.5):\n",
    "        self.max_level = max_level  # Maximum level of the skip list\n",
    "        self.p = p  # Probability of promoting to next level\n",
    "        self.level = 0  # Current maximum level of skip list\n",
    "        \n",
    "        # Create head node with key set to None (will be smaller than all real keys)\n",
    "        self.head = SkipNode(None, None, max_level)\n",
    "    \n",
    "    def random_level(self) -> int:\n",
    "        \"\"\"Randomly determine the level for a new node\"\"\"\n",
    "        level = 0\n",
    "        while random.random() < self.p and level < self.max_level:\n",
    "            level += 1\n",
    "        return level\n",
    "    \n",
    "    def search(self, key: Any) -> Optional[Any]:\n",
    "        \"\"\"Search for a key in the skip list\"\"\"\n",
    "        current = self.head\n",
    "        \n",
    "        # Start from the highest level and work down\n",
    "        for i in range(self.level, -1, -1):\n",
    "            # Move forward at the current level as far as possible\n",
    "            while current.forward[i] and current.forward[i].key < key:\n",
    "                current = current.forward[i]\n",
    "        \n",
    "        # Move to the node right after the last smaller key\n",
    "        current = current.forward[0]\n",
    "        \n",
    "        # Return the value if the key matches, otherwise None\n",
    "        if current and current.key == key:\n",
    "            return current.value\n",
    "        return None\n",
    "    \n",
    "    def insert(self, key: Any, value: Any):\n",
    "        \"\"\"Insert a new key-value pair into the skip list\"\"\"\n",
    "        # Array to track updates at each level\n",
    "        update = [None] * (self.max_level + 1)\n",
    "        current = self.head\n",
    "        \n",
    "        # Find the position to insert the new node\n",
    "        for i in range(self.level, -1, -1):\n",
    "            while current.forward[i] and current.forward[i].key < key:\n",
    "                current = current.forward[i]\n",
    "            update[i] = current\n",
    "        \n",
    "        # Move to the next node\n",
    "        current = current.forward[0]\n",
    "        \n",
    "        # If key already exists, update the value\n",
    "        if current and current.key == key:\n",
    "            if isinstance(current.value, list):\n",
    "                current.value.append(value)\n",
    "            else:\n",
    "                current.value = [current.value, value]\n",
    "            return\n",
    "        \n",
    "        # Generate a random level for the new node\n",
    "        new_level = self.random_level()\n",
    "        \n",
    "        # Update the skip list's level if the new level is higher\n",
    "        if new_level > self.level:\n",
    "            for i in range(self.level + 1, new_level + 1):\n",
    "                update[i] = self.head\n",
    "            self.level = new_level\n",
    "        \n",
    "        # Create a new node\n",
    "        new_node = SkipNode(key, value, new_level)\n",
    "        \n",
    "        # Insert the new node by updating the forward links\n",
    "        for i in range(new_level + 1):\n",
    "            new_node.forward[i] = update[i].forward[i]\n",
    "            update[i].forward[i] = new_node\n",
    "\n",
    "\n",
    "class HybridSkipHashIndex:\n",
    "    \"\"\"\n",
    "    A hybrid index that uses hash table for initial bucketing and skip lists within each bucket.\n",
    "    This combines O(1) lookup of hash tables with the ordered structure benefits of skip lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_bits: int = 8, max_level: int = 16, p: float = 0.5):\n",
    "        self.bucket_bits = bucket_bits  # Determines number of buckets (2^bucket_bits)\n",
    "        self.num_buckets = 1 << bucket_bits  # 2^bucket_bits\n",
    "        self.buckets = [SkipList(max_level, p) for _ in range(self.num_buckets)]\n",
    "    \n",
    "    def _hash_key(self, key: Any) -> int:\n",
    "        \"\"\"Hash function to determine the bucket\"\"\"\n",
    "        # Use Python's hash function and mask to get bucket index\n",
    "        hash_value = hash(key) & 0x7FFFFFFF  # Ensure positive value\n",
    "        return hash_value % self.num_buckets\n",
    "    \n",
    "    def insert(self, key: Any, value: Any):\n",
    "        \"\"\"Insert a key-value pair into the appropriate bucket's skip list\"\"\"\n",
    "        bucket_idx = self._hash_key(key)\n",
    "        self.buckets[bucket_idx].insert(key, value)\n",
    "    \n",
    "    def search(self, key: Any) -> Optional[Any]:\n",
    "        \"\"\"Search for a key in the appropriate bucket's skip list\"\"\"\n",
    "        bucket_idx = self._hash_key(key)\n",
    "        return self.buckets[bucket_idx].search(key)\n",
    "    \n",
    "    def build_index(self, data: List[Dict], key_field: str):\n",
    "        \"\"\"Build the hybrid index from a list of dictionaries\"\"\"\n",
    "        for record in data:\n",
    "            key = record[key_field]\n",
    "            self.insert(key, record)\n",
    "\n",
    "\n",
    "def hybrid_skip_hash_join(left_data: List[Dict], right_data: List[Dict], \n",
    "                         left_key: str, right_key: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform an inner join using a hybrid hash/skip list index on the right table.\n",
    "    This approach combines the constant-time initial lookup of hash tables\n",
    "    with the ordered structure benefits of skip lists.\n",
    "    \"\"\"\n",
    "    # Build hybrid index on the right table\n",
    "    start_build = time.time()\n",
    "    \n",
    "    # Determine appropriate bucket bits based on table size\n",
    "    # More buckets for larger tables to keep skip lists small\n",
    "    bucket_bits = min(12, max(8, int(np.log2(len(right_data) / 100))))\n",
    "    index = HybridSkipHashIndex(bucket_bits=bucket_bits)\n",
    "    \n",
    "    for record in right_data:\n",
    "        key = record[right_key]\n",
    "        index.insert(key, record)\n",
    "    \n",
    "    build_time = time.time() - start_build\n",
    "    print(f\"Hybrid index build time: {build_time:.6f} seconds\")\n",
    "    \n",
    "    # Perform the join by searching the hybrid index for each record in the left table\n",
    "    start_join = time.time()\n",
    "    result = []\n",
    "    \n",
    "    for left_record in left_data:\n",
    "        left_val = left_record[left_key]\n",
    "        right_records = index.search(left_val)\n",
    "        \n",
    "        if right_records:\n",
    "            # Handle case where we have multiple matches\n",
    "            if isinstance(right_records, list):\n",
    "                for right_record in right_records:\n",
    "                    joined_record = {**left_record, **right_record}\n",
    "                    result.append(joined_record)\n",
    "            else:\n",
    "                joined_record = {**left_record, **right_records}\n",
    "                result.append(joined_record)\n",
    "    \n",
    "    join_time = time.time() - start_join\n",
    "    print(f\"Hybrid join time: {join_time:.6f} seconds\")\n",
    "    print(f\"Total hybrid join time: {build_time + join_time:.6f} seconds\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def multi_threaded_hash_join(left_data: List[Dict], right_data: List[Dict], \n",
    "                            left_key: str, right_key: str, num_threads: int = 4) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform a hash join using multiple threads for parallelization.\n",
    "    This is included for comparison with the hybrid approach.\n",
    "    \"\"\"\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    import threading\n",
    "    \n",
    "    # Build hash table index on the right table\n",
    "    start_build = time.time()\n",
    "    hash_table = {}\n",
    "    \n",
    "    for record in right_data:\n",
    "        key = record[right_key]\n",
    "        if key in hash_table:\n",
    "            hash_table[key].append(record)\n",
    "        else:\n",
    "            hash_table[key] = [record]\n",
    "    \n",
    "    build_time = time.time() - start_build\n",
    "    print(f\"MT Hash table build time: {build_time:.6f} seconds\")\n",
    "    \n",
    "    # Split left data for parallel processing\n",
    "    chunk_size = (len(left_data) + num_threads - 1) // num_threads\n",
    "    chunks = [left_data[i:i+chunk_size] for i in range(0, len(left_data), chunk_size)]\n",
    "    \n",
    "    # Shared result list with lock for thread safety\n",
    "    result = []\n",
    "    result_lock = threading.Lock()\n",
    "    \n",
    "    def process_chunk(chunk):\n",
    "        \"\"\"Process a chunk of the left table and add results to the shared result list\"\"\"\n",
    "        local_result = []\n",
    "        for left_record in chunk:\n",
    "            left_val = left_record[left_key]\n",
    "            if left_val in hash_table:\n",
    "                for right_record in hash_table[left_val]:\n",
    "                    joined_record = {**left_record, **right_record}\n",
    "                    local_result.append(joined_record)\n",
    "        \n",
    "        # Add local results to shared result list\n",
    "        with result_lock:\n",
    "            result.extend(local_result)\n",
    "    \n",
    "    # Perform the join in parallel\n",
    "    start_join = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        executor.map(process_chunk, chunks)\n",
    "    \n",
    "    join_time = time.time() - start_join\n",
    "    print(f\"MT Hash join time: {join_time:.6f} seconds\")\n",
    "    print(f\"Total MT hash join time: {build_time + join_time:.6f} seconds\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def bloom_filter_hash_join(left_data: List[Dict], right_data: List[Dict], \n",
    "                          left_key: str, right_key: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform a hash join using a Bloom filter to quickly filter out non-matching records.\n",
    "    This is another optimization strategy for comparison.\n",
    "    \"\"\"\n",
    "    from bitarray import bitarray\n",
    "    import hashlib\n",
    "    \n",
    "    # Parameters for Bloom filter\n",
    "    m = 1 << 20  # Size of bit array (1 million bits)\n",
    "    k = 7  # Number of hash functions\n",
    "    \n",
    "    def bloom_hash(key, seed):\n",
    "        \"\"\"Hash function for Bloom filter\"\"\"\n",
    "        key_str = str(key) + str(seed)\n",
    "        hash_val = int(hashlib.md5(key_str.encode()).hexdigest(), 16)\n",
    "        return hash_val % m\n",
    "    \n",
    "    # Build Bloom filter and hash table\n",
    "    start_build = time.time()\n",
    "    \n",
    "    # Create Bloom filter\n",
    "    bloom = bitarray(m)\n",
    "    bloom.setall(0)\n",
    "    \n",
    "    # Add right keys to Bloom filter\n",
    "    right_keys = set()\n",
    "    for record in right_data:\n",
    "        key = record[right_key]\n",
    "        right_keys.add(key)\n",
    "        for i in range(k):\n",
    "            bloom[bloom_hash(key, i)] = 1\n",
    "    \n",
    "    # Build hash table for right table\n",
    "    hash_table = {}\n",
    "    for record in right_data:\n",
    "        key = record[right_key]\n",
    "        if key in hash_table:\n",
    "            hash_table[key].append(record)\n",
    "        else:\n",
    "            hash_table[key] = [record]\n",
    "    \n",
    "    build_time = time.time() - start_build\n",
    "    print(f\"Bloom filter + hash table build time: {build_time:.6f} seconds\")\n",
    "    \n",
    "    # Perform the join using Bloom filter for initial filtering\n",
    "    start_join = time.time()\n",
    "    result = []\n",
    "    bloom_hits = 0\n",
    "    actual_matches = 0\n",
    "    \n",
    "    for left_record in left_data:\n",
    "        left_val = left_record[left_key]\n",
    "        \n",
    "        # Check Bloom filter first\n",
    "        is_potential_match = True\n",
    "        for i in range(k):\n",
    "            if not bloom[bloom_hash(left_val, i)]:\n",
    "                is_potential_match = False\n",
    "                break\n",
    "        \n",
    "        if is_potential_match:\n",
    "            bloom_hits += 1\n",
    "            # Potential match, check hash table\n",
    "            if left_val in hash_table:\n",
    "                actual_matches += 1\n",
    "                for right_record in hash_table[left_val]:\n",
    "                    joined_record = {**left_record, **right_record}\n",
    "                    result.append(joined_record)\n",
    "    \n",
    "    join_time = time.time() - start_join\n",
    "    print(f\"Bloom filter join time: {join_time:.6f} seconds\")\n",
    "    print(f\"Total Bloom filter join time: {build_time + join_time:.6f} seconds\")\n",
    "    print(f\"Bloom filter hits: {bloom_hits}, Actual matches: {actual_matches}, False positives: {bloom_hits - actual_matches}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_test_data(size_left: int, size_right: int, \n",
    "                      key_range: int, seed: int = 42) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Generate test data for join operations\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    left_data = []\n",
    "    right_data = []\n",
    "    \n",
    "    # Generate left table\n",
    "    for i in range(size_left):\n",
    "        record = {\n",
    "            'id': i,\n",
    "            'join_key': random.randint(1, key_range),\n",
    "            'value_left': f\"left_value_{i}\"\n",
    "        }\n",
    "        left_data.append(record)\n",
    "    \n",
    "    # Generate right table\n",
    "    for i in range(size_right):\n",
    "        record = {\n",
    "            'id': i,\n",
    "            'join_key': random.randint(1, key_range),\n",
    "            'value_right': f\"right_value_{i}\"\n",
    "        }\n",
    "        right_data.append(record)\n",
    "    \n",
    "    return left_data, right_data\n",
    "\n",
    "\n",
    "def test_advanced_joins():\n",
    "    \"\"\"\n",
    "    Test advanced join algorithms with various data sizes and distributions\n",
    "    \"\"\"\n",
    "    print(\"=== Medium Dataset Test ===\")\n",
    "    left_data, right_data = generate_test_data(10000, 10000, 5000)\n",
    "    \n",
    "    hybrid_result = hybrid_skip_hash_join(left_data, right_data, 'join_key', 'join_key')\n",
    "    mt_hash_result = multi_threaded_hash_join(left_data, right_data, 'join_key', 'join_key')\n",
    "    try:\n",
    "        bloom_result = bloom_filter_hash_join(left_data, right_data, 'join_key', 'join_key')\n",
    "        print(f\"Bloom filter join result size: {len(bloom_result)}\")\n",
    "    except ImportError:\n",
    "        print(\"Bloom filter join requires the 'bitarray' package. Skipping this test.\")\n",
    "    \n",
    "    print(f\"Hybrid join result size: {len(hybrid_result)}\")\n",
    "    print(f\"Multi-threaded hash join result size: {len(mt_hash_result)}\")\n",
    "    \n",
    "    print(\"\\n=== Large Dataset Test ===\")\n",
    "    left_data, right_data = generate_test_data(100000, 100000, 50000)\n",
    "    \n",
    "    hybrid_result = hybrid_skip_hash_join(left_data, right_data, 'join_key', 'join_key')\n",
    "    mt_hash_result = multi_threaded_hash_join(left_data, right_data, 'join_key', 'join_key')\n",
    "    try:\n",
    "        bloom_result = bloom_filter_hash_join(left_data, right_data, 'join_key', 'join_key')\n",
    "        print(f\"Bloom filter join result size: {len(bloom_result)}\")\n",
    "    except ImportError:\n",
    "        print(\"Bloom filter join requires the 'bitarray' package. Skipping this test.\")\n",
    "    \n",
    "    print(f\"Hybrid join result size: {len(hybrid_result)}\")\n",
    "    print(f\"Multi-threaded hash join result size: {len(mt_hash_result)}\")\n",
    "    \n",
    "    # Test with very skewed data distribution\n",
    "    print(\"\\n=== Highly Skewed Data Distribution Test ===\")\n",
    "    \n",
    "    # Create skewed data with 90% of keys in a tiny range\n",
    "    left_skewed = []\n",
    "    right_skewed = []\n",
    "    \n",
    "    for i in range(10000):\n",
    "        skewed_key = random.randint(1, 10) if random.random() < 0.9 else random.randint(11, 5000)\n",
    "        left_record = {\n",
    "            'id': i,\n",
    "            'join_key': skewed_key,\n",
    "            'value_left': f\"left_value_{i}\"\n",
    "        }\n",
    "        left_skewed.append(left_record)\n",
    "        \n",
    "        skewed_key = random.randint(1, 10) if random.random() < 0.9 else random.randint(11, 5000)\n",
    "        right_record = {\n",
    "            'id': i,\n",
    "            'join_key': skewed_key,\n",
    "            'value_right': f\"right_value_{i}\"\n",
    "        }\n",
    "        right_skewed.append(right_record)\n",
    "    \n",
    "    hybrid_result = hybrid_skip_hash_join(left_skewed, right_skewed, 'join_key', 'join_key')\n",
    "    mt_hash_result = multi_threaded_hash_join(left_skewed, right_skewed, 'join_key', 'join_key')\n",
    "    try:\n",
    "        bloom_result = bloom_filter_hash_join(left_skewed, right_skewed, 'join_key', 'join_key')\n",
    "        print(f\"Bloom filter join result size: {len(bloom_result)}\")\n",
    "    except ImportError:\n",
    "        print(\"Bloom filter join requires the 'bitarray' package. Skipping this test.\")\n",
    "    \n",
    "    print(f\"Hybrid join result size: {len(hybrid_result)}\")\n",
    "    print(f\"Multi-threaded hash join result size: {len(mt_hash_result)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_advanced_joins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfcb5aa-3f1b-4aba-b6aa-f28d626225b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Data Science)",
   "language": "python",
   "name": "py311ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
